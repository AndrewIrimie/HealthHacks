# realtime_offline.py - Fully Offline Real-time STT + LLM Pipeline
import asyncio
import threading
import queue
import numpy as np
import sounddevice as sd
import webrtcvad
import time
import requests
import json
import uuid
import pickle
import os
from datetime import datetime
from difflib import SequenceMatcher
from faster_whisper import WhisperModel

# prompt = '''

#     You are an AI assistant that is inside of a dictation machine. A doctor will be dictating notes to you. Your job is to listen to the doctors notes and 


# '''

class OfflineRealtimePipeline:
    def __init__(self):
        print("üîß Initializing Offline Real-time Pipeline...")
        
        # Audio settings
        self.sample_rate = 16000
        self.chunk_duration = 3.0  # seconds
        self.chunk_size = int(self.sample_rate * self.chunk_duration)
        
        # Continuous buffer settings
        self.buffer_duration = 30.0  # 30-second rolling buffer
        self.buffer_size = int(self.sample_rate * self.buffer_duration)
        self.continuous_buffer = np.zeros(self.buffer_size, dtype=np.float32)
        self.buffer_index = 0
        self.buffer_lock = threading.Lock()
        
        # Audio processing
        self.raw_audio_queue = queue.Queue()  # Single audio processing queue
        self.is_recording = False
        
        # Thread synchronization
        self.utterance_lock = threading.Lock()
        
        # Deduplication settings
        self.recent_transcripts = []
        self.last_response_time = 0
        self.min_response_gap = 2.0  # seconds between responses
        self.similarity_threshold = 0.8  # similarity threshold for deduplication
        
        # Persistent audio storage - NEVER lose audio
        self.session_id = str(uuid.uuid4())[:8]
        self.audio_log_dir = f"audio_sessions/{self.session_id}"
        os.makedirs(self.audio_log_dir, exist_ok=True)
        self.audio_segments = {}  # segment_id -> AudioSegment
        self.utterance_queue = {}  # utterance_id -> UtteranceRecord
        self.processing_status = {}  # track every utterance through pipeline
        self.next_segment_id = 1
        self.audio_log_lock = threading.Lock()
        
        # LLM processing - NO DROPPING, only queuing
        self.last_llm_request_time = 0
        self.min_llm_gap = 3.0  # minimum seconds between LLM requests
        self.llm_request_consolidation_window = 2.0  # seconds to wait for consolidation
        self.pending_llm_requests = []  # Will NEVER be cleared, only processed
        self.llm_health_status = "healthy"  # healthy, degraded, failed
        self.llm_error_count = 0
        self.llm_consecutive_errors = 0
        self.max_consecutive_errors = 3
        self.llm_backlog = []  # Persistent storage for failed requests
        self.medical_mode = True  # Zero-loss medical dictation mode
        
        # Storage management settings
        self.storage_mode = "standard"  # medical, standard, minimal
        self.max_session_age_hours = 24  # Auto-delete sessions older than this
        self.max_total_storage_mb = 1000  # Max storage before cleanup (1GB default)
        self.cleanup_interval_minutes = 30  # How often to run cleanup
        self.keep_raw_chunks = False  # Keep individual raw audio chunks after processing
        self.compress_audio = True  # Use compression for stored audio
        self.last_cleanup_time = 0
        self.storage_stats = {
            'total_sessions': 0,
            'total_storage_mb': 0,
            'cleanup_count': 0,
            'files_deleted': 0
        }
        
        # VAD settings
        self.vad = webrtcvad.Vad(2)  # Higher aggressiveness for noise rejection
        
        # Audio energy settings
        self.background_noise_level = 0.01  # Will be calibrated
        self.energy_threshold_multiplier = 3.0  # Energy must be 3x background noise
        self.min_energy_threshold = 0.005  # Absolute minimum energy threshold
        self.energy_history = []  # For background noise estimation
        self.calibration_frames = 0
        self.max_calibration_frames = 100  # ~6 seconds of calibration
        
        # Pause detection settings
        self.pause_threshold = 5  # seconds of silence to consider a pause
        self.min_utterance_length = 0.8  # minimum utterance length in seconds (increased)
        self.silence_frames_count = 0
        self.speech_frames_count = 0
        self.current_utterance_start = 0
        self.is_in_utterance = False
        
        # Speech detection smoothing
        self.speech_confidence_history = []
        self.confidence_window_size = 5  # frames to average
        self.min_consecutive_speech = 3  # minimum consecutive speech frames
        
        # Initialize models
        self._init_stt()
        self._init_llm()
        
        # Processing queues
        self.stt_queue = queue.Queue()
        self.llm_queue = queue.Queue()
        
        # Configure storage mode based on medical requirements
        if self.medical_mode:
            self.storage_mode = "medical"
            self.max_session_age_hours = 7 * 24  # Keep medical sessions for 7 days
            self.keep_raw_chunks = True  # Keep everything in medical mode
            
        print("‚úÖ Pipeline initialized successfully!")
        print(f"üìÅ Audio session: {self.session_id}")
        print(f"üè• Medical mode: {'ENABLED' if self.medical_mode else 'DISABLED'} - Zero audio loss guaranteed")
        print(f"üíæ Storage mode: {self.storage_mode.upper()} (retention: {self.max_session_age_hours}h, limit: {self.max_total_storage_mb}MB)")
        
    class AudioSegment:
        """Persistent audio segment with metadata"""
        def __init__(self, segment_id, audio_data, timestamp):
            self.segment_id = segment_id
            self.audio_data = audio_data.copy()  # Always copy to prevent modification
            self.timestamp = timestamp
            self.datetime = datetime.fromtimestamp(timestamp)
            self.duration = len(audio_data) / 16000  # assuming 16kHz
            self.processed = False
            self.transcription = None
            self.llm_response = None
            self.error_log = []
    
    class UtteranceRecord:
        """Complete utterance tracking from detection to LLM response"""
        def __init__(self, utterance_id, start_time):
            self.utterance_id = utterance_id
            self.start_time = start_time
            self.end_time = None
            self.audio_segment_ids = []
            self.raw_transcription = None
            self.cleaned_transcription = None
            self.llm_request_time = None
            self.llm_response_time = None
            self.llm_response = None
            self.status = "detecting"  # detecting, transcribing, llm_queued, llm_processing, completed, error
            self.error_log = []
            self.retry_count = 0
    
    def _init_stt(self):
        """Initialize Faster-Whisper for offline STT"""
        print("üìù Loading Whisper model (this may take a few minutes on first run)...")
        print("‚è≥ Downloading model files if needed...")
        # Use small model for speed, can change to 'base' or 'medium' for better accuracy
        try:
            self.whisper_model = WhisperModel("small", device="cpu", compute_type="int8")
            print("‚úÖ Whisper model loaded successfully")
        except Exception as e:
            print(f"‚ùå Failed to load Whisper small model: {e}")
            print("üí° Trying with base model...")
            self.whisper_model = WhisperModel("base", device="cpu", compute_type="int8")
            print("‚úÖ Whisper model loaded successfully")
    
    def _init_llm(self):
        """Initialize Ollama LLM"""
        print("üß† Connecting to Ollama...")
        self.ollama_url = "http://localhost:11434/api/generate"
        self.model_name = "llama3.2:latest"  # Fast, decent quality model
        
        # Test connection
        try:
            response = requests.post(self.ollama_url, 
                json={"model": self.model_name, "prompt": "test", "stream": False},
                timeout=30)
            if response.status_code == 200:
                print("‚úÖ Ollama connected")
            else:
                print("‚ùå Ollama connection failed")
                raise Exception("Ollama not available")
        except Exception as e:
            print(f"‚ùå Ollama error: {e}")
            print("Make sure Ollama is running with: ollama serve")
            raise
    
    
    def audio_callback(self, indata, frames, time_info, status):
        """Real-time audio input callback"""
        if status:
            print(f"Audio status: {status}")
        
        if self.is_recording:
            # Convert to float32 and add to processing queue
            audio_data = indata.flatten().astype(np.float32)
            
            # Add to single audio processing queue
            self.raw_audio_queue.put(audio_data.copy())
            
            # Update continuous circular buffer for recovery
            with self.buffer_lock:
                data_len = len(audio_data)
                if self.buffer_index + data_len <= self.buffer_size:
                    # Fits in current position
                    self.continuous_buffer[self.buffer_index:self.buffer_index + data_len] = audio_data
                    self.buffer_index += data_len
                else:
                    # Wrap around
                    remaining = self.buffer_size - self.buffer_index
                    self.continuous_buffer[self.buffer_index:] = audio_data[:remaining]
                    overflow = data_len - remaining
                    if overflow > 0:
                        self.continuous_buffer[:overflow] = audio_data[remaining:]
                    self.buffer_index = overflow
                
                # Reset index if we've filled the buffer
                if self.buffer_index >= self.buffer_size:
                    self.buffer_index = 0
    
    def start_audio_stream(self):
        """Start real-time audio capture"""
        print("üé§ Starting audio stream...")
        self.stream = sd.InputStream(
            samplerate=self.sample_rate,
            channels=1,
            dtype=np.float32,
            callback=self.audio_callback,
            blocksize=1024,
        )
        self.stream.start()
        self.is_recording = True
        print("‚úÖ Audio stream started")
    
    def stop_audio_stream(self):
        """Stop audio capture"""
        if hasattr(self, 'stream'):
            self.is_recording = False
            self.stream.stop()
            self.stream.close()
            print("üõë Audio stream stopped")
    
    def has_voice_activity(self, audio_chunk):
        """Multi-factor speech detection with noise rejection"""
        try:
            # First check: Energy gate - fast rejection of low-energy noise
            if not self.passes_energy_gate(audio_chunk):
                return False
            
            # Second check: VAD analysis
            audio_int16 = (audio_chunk * 32767).astype(np.int16)
            
            # Split into 30ms frames for VAD (480 samples at 16kHz)
            frame_size = 480
            frames = [audio_int16[i:i+frame_size] for i in range(0, len(audio_int16), frame_size)]
            
            # Count speech frames detected by VAD
            speech_frames = 0
            total_frames = 0
            for frame in frames:
                if len(frame) == frame_size:
                    total_frames += 1
                    try:
                        if self.vad.is_speech(frame.tobytes(), self.sample_rate):
                            speech_frames += 1
                    except:
                        continue
            
            if total_frames == 0:
                return False
            
            # Higher threshold - require at least 30% of frames to be speech
            vad_confidence = speech_frames / total_frames
            vad_passes = vad_confidence >= 0.3
            
            # Third check: Spectral analysis for speech-like characteristics
            spectral_centroid = self.calculate_spectral_centroid(audio_chunk)
            # Human speech typically has spectral centroid between 500-3000 Hz
            spectral_passes = 500 <= spectral_centroid <= 3000
            
            # Calculate overall confidence score
            energy = self.calculate_rms_energy(audio_chunk)
            energy_ratio = energy / max(self.background_noise_level, 0.001)
            
            # Combine factors into confidence score
            confidence = 0.0
            if vad_passes:
                confidence += 0.4 * vad_confidence
            if spectral_passes:
                confidence += 0.3
            if energy_ratio > 2.0:
                confidence += 0.3 * min(energy_ratio / 10.0, 1.0)
            
            # Add to confidence history for smoothing
            self.speech_confidence_history.append(confidence)
            if len(self.speech_confidence_history) > self.confidence_window_size:
                self.speech_confidence_history.pop(0)
            
            # Smooth confidence over recent frames
            smoothed_confidence = np.mean(self.speech_confidence_history)
            
            # Final decision: require significant confidence
            is_speech = smoothed_confidence >= 0.5
            
            # Debug output during calibration
            if self.calibration_frames < self.max_calibration_frames:
                if self.calibration_frames % 20 == 0:  # Every ~1.3 seconds
                    print(f"üîß [CALIBRATION] Energy: {energy:.6f}, VAD: {vad_confidence:.2f}, Spectral: {spectral_centroid:.0f}Hz, Confidence: {smoothed_confidence:.2f}")
            
            return is_speech
            
        except Exception as e:
            print(f"‚ö†Ô∏è [VAD] Error in voice activity detection: {e}")
            return False  # Fail safe - reject on error
    
    def detect_utterance_boundary(self, audio_chunk):
        """Detect utterance boundaries with thread-safe state management"""
        has_speech = self.has_voice_activity(audio_chunk)
        
        # Each chunk is typically 1024 samples at 16kHz = ~64ms
        frame_duration = len(audio_chunk) / self.sample_rate
        frames_per_second = 1.0 / frame_duration
        
        with self.utterance_lock:  # Thread-safe state management
            if has_speech:
                self.speech_frames_count += 1
                self.silence_frames_count = 0
                
                # Require minimum consecutive speech frames before starting utterance
                if not self.is_in_utterance and self.speech_frames_count >= self.min_consecutive_speech:
                    # Start of new utterance - confirmed by consecutive speech frames
                    self.is_in_utterance = True
                    self.current_utterance_start = time.time()
                    print("üéôÔ∏è [VOICE] Speech detected - starting utterance...")
                    return "utterance_start"
                    
            else:
                self.silence_frames_count += 1
                
                # Reset speech count if we don't have enough consecutive frames yet
                if not self.is_in_utterance:
                    self.speech_frames_count = 0
                
                # Check if we've had enough silence to end utterance
                silence_duration = self.silence_frames_count / frames_per_second
                
                if self.is_in_utterance and silence_duration >= self.pause_threshold:
                    # End of utterance
                    utterance_duration = time.time() - self.current_utterance_start
                    self.is_in_utterance = False
                    self.speech_frames_count = 0  # Reset for next utterance
                    
                    # Only consider it a valid utterance if it was long enough
                    if utterance_duration >= self.min_utterance_length:
                        print(f"üéôÔ∏è [VOICE] Utterance complete ({utterance_duration:.1f}s)")
                        return "utterance_end"
                    else:
                        print(f"üéôÔ∏è [VOICE] Utterance too short ({utterance_duration:.1f}s), ignoring")
                        return "utterance_too_short"
            
            return "continuing" if self.is_in_utterance else "silence"
    
    def save_audio_segment(self, audio_data, metadata=None):
        """Persistently save audio segment with guaranteed storage"""
        with self.audio_log_lock:
            segment_id = self.next_segment_id
            self.next_segment_id += 1
            
            timestamp = time.time()
            segment = self.AudioSegment(segment_id, audio_data, timestamp)
            
            # Store in memory
            self.audio_segments[segment_id] = segment
            
            # Save to disk for persistence
            segment_file = os.path.join(self.audio_log_dir, f"segment_{segment_id:06d}.pkl")
            try:
                with open(segment_file, 'wb') as f:
                    pickle.dump({
                        'segment_id': segment_id,
                        'audio_data': audio_data,
                        'timestamp': timestamp,
                        'metadata': metadata or {}
                    }, f)
                
                # Also save as WAV for human verification
                wav_file = os.path.join(self.audio_log_dir, f"segment_{segment_id:06d}.wav")
                self._save_wav(audio_data, wav_file)
                
                # print(f"üíæ [STORAGE] Saved audio segment {segment_id} ({segment.duration:.1f}s)")
                return segment_id
                
            except Exception as e:
                print(f"‚ö†Ô∏è [STORAGE] Failed to save segment {segment_id}: {e}")
                # Keep in memory even if disk save fails
                return segment_id
    
    def _save_wav(self, audio_data, filename):
        """Save audio data as WAV file for human verification"""
        try:
            import soundfile as sf
            sf.write(filename, audio_data, self.sample_rate)
        except ImportError:
            # Fallback - save as numpy array
            np.save(filename.replace('.wav', '.npy'), audio_data)
        except Exception as e:
            print(f"‚ö†Ô∏è [STORAGE] Could not save WAV: {e}")
    
    def create_utterance_record(self, utterance_id=None):
        """Create new utterance record with guaranteed tracking"""
        if utterance_id is None:
            utterance_id = f"utt_{int(time.time() * 1000)}_{len(self.utterance_queue)}"
        
        with self.audio_log_lock:
            record = self.UtteranceRecord(utterance_id, time.time())
            self.utterance_queue[utterance_id] = record
            self.processing_status[utterance_id] = "created"
            
            print(f"üìã [TRACK] Created utterance record {utterance_id}")
            return utterance_id
    
    def update_utterance_status(self, utterance_id, status, details=None):
        """Update utterance status with logging"""
        with self.audio_log_lock:
            if utterance_id in self.utterance_queue:
                record = self.utterance_queue[utterance_id]
                old_status = record.status
                record.status = status
                self.processing_status[utterance_id] = status
                
                if details:
                    if hasattr(details, 'keys'):  # dict-like
                        for key, value in details.items():
                            setattr(record, key, value)
                
                print(f"üìä [TRACK] Utterance {utterance_id}: {old_status} ‚Üí {status}")
            else:
                print(f"‚ö†Ô∏è [TRACK] Unknown utterance ID: {utterance_id}")
    
    def get_processing_summary(self):
        """Get summary of all audio processing status"""
        with self.audio_log_lock:
            total_segments = len(self.audio_segments)
            total_utterances = len(self.utterance_queue)
            
            status_counts = {}
            for status in self.processing_status.values():
                status_counts[status] = status_counts.get(status, 0) + 1
            
            return {
                'session_id': self.session_id,
                'total_audio_segments': total_segments,
                'total_utterances': total_utterances,
                'status_breakdown': status_counts,
                'session_duration': time.time() - (min(seg.timestamp for seg in self.audio_segments.values()) if self.audio_segments else time.time())
            }
    
    def calculate_rms_energy(self, audio_chunk):
        """Calculate RMS energy of audio chunk"""
        try:
            return np.sqrt(np.mean(np.square(audio_chunk.astype(np.float32))))
        except:
            return 0.0
    
    def calculate_spectral_centroid(self, audio_chunk):
        """Calculate spectral centroid to help distinguish speech from noise"""
        try:
            # Simple spectral centroid calculation
            fft = np.fft.rfft(audio_chunk)
            magnitude = np.abs(fft)
            freqs = np.fft.rfftfreq(len(audio_chunk), 1.0/self.sample_rate)
            
            if np.sum(magnitude) == 0:
                return 0
            
            centroid = np.sum(freqs * magnitude) / np.sum(magnitude)
            return centroid
        except:
            return 0.0
    
    def update_background_noise(self, energy_level):
        """Update background noise estimation during calibration and quiet periods"""
        if self.calibration_frames < self.max_calibration_frames:
            # Still calibrating
            self.energy_history.append(energy_level)
            self.calibration_frames += 1
            
            if self.calibration_frames == self.max_calibration_frames:
                # Calibration complete
                if self.energy_history:
                    self.background_noise_level = np.percentile(self.energy_history, 75)  # Use 75th percentile
                    print(f"üîß [CALIBRATION] Background noise level: {self.background_noise_level:.6f}")
                else:
                    self.background_noise_level = self.min_energy_threshold
        else:
            # Ongoing noise level updates during silence
            if not self.is_in_utterance:
                # Update background noise level with exponential smoothing
                alpha = 0.01  # Slow adaptation
                self.background_noise_level = alpha * energy_level + (1 - alpha) * self.background_noise_level
    
    def passes_energy_gate(self, audio_chunk):
        """Check if audio chunk passes energy-based filtering"""
        energy = self.calculate_rms_energy(audio_chunk)
        self.update_background_noise(energy)
        
        # Dynamic threshold based on background noise
        energy_threshold = max(
            self.background_noise_level * self.energy_threshold_multiplier,
            self.min_energy_threshold
        )
        
        return energy > energy_threshold
    
    def get_recent_audio_buffer(self, duration_seconds=10.0):
        """Get recent audio from the continuous buffer for recovery purposes"""
        with self.buffer_lock:
            samples_needed = int(duration_seconds * self.sample_rate)
            samples_needed = min(samples_needed, self.buffer_size)
            
            if self.buffer_index >= samples_needed:
                # Simple case - no wrap around
                return self.continuous_buffer[self.buffer_index - samples_needed:self.buffer_index].copy()
            else:
                # Wrap around case
                part1_size = samples_needed - self.buffer_index
                part1 = self.continuous_buffer[-part1_size:] if part1_size > 0 else np.array([])
                part2 = self.continuous_buffer[:self.buffer_index] if self.buffer_index > 0 else np.array([])
                return np.concatenate([part1, part2]) if len(part1) > 0 or len(part2) > 0 else np.array([])
    
    def is_similar_transcript(self, new_text):
        """Check if transcript is similar to recent ones"""
        if not new_text or len(new_text.strip()) < 3:
            return True
        
        new_text = new_text.lower().strip()
        
        for recent_text, timestamp in self.recent_transcripts:
            # Remove old transcripts (older than 10 seconds)
            if time.time() - timestamp > 10:
                continue
            
            similarity = SequenceMatcher(None, new_text, recent_text.lower()).ratio()
            if similarity > self.similarity_threshold:
                return True
        
        return False
    
    def add_to_recent_transcripts(self, text):
        """Add transcript to recent list with timestamp"""
        current_time = time.time()
        self.recent_transcripts.append((text, current_time))
        
        # Keep only recent transcripts (last 10 seconds)
        self.recent_transcripts = [(t, ts) for t, ts in self.recent_transcripts 
                                 if current_time - ts <= 10]
    
    def process_raw_audio(self):
        """Process raw audio with persistent storage and zero-loss guarantee"""
        utterance_buffer = []
        current_utterance_id = None
        
        while self.is_recording:
            try:
                # Get raw audio data
                audio_chunk = self.raw_audio_queue.get(timeout=0.1)
                
                # Save raw audio chunks based on storage policy
                if self.storage_mode == "medical" or self.keep_raw_chunks:
                    segment_id = self.save_audio_segment(audio_chunk, {"type": "raw_chunk"})
                else:
                    # In standard/minimal mode, only keep utterances, not every chunk
                    segment_id = None
                
                utterance_buffer.extend(audio_chunk)
                
                # Detect utterance boundaries
                boundary_type = self.detect_utterance_boundary(audio_chunk)
                
                if boundary_type == "utterance_start":
                    # Create new utterance record
                    current_utterance_id = self.create_utterance_record()
                    self.update_utterance_status(current_utterance_id, "detecting")
                    
                elif boundary_type == "utterance_end":
                    # Process complete utterance with guaranteed storage
                    if utterance_buffer and current_utterance_id:
                        # Add silence padding
                        padding_samples = int(0.5 * self.sample_rate)
                        padded_audio = np.concatenate([
                            np.zeros(padding_samples),
                            np.array(utterance_buffer),
                            np.zeros(padding_samples)
                        ])
                        
                        # Save complete utterance permanently
                        utterance_segment_id = self.save_audio_segment(
                            padded_audio, 
                            {"type": "complete_utterance", "utterance_id": current_utterance_id}
                        )
                        
                        # Update utterance record
                        self.update_utterance_status(current_utterance_id, "transcribing", {
                            'end_time': time.time(),
                            'audio_segment_ids': [utterance_segment_id],
                            'duration': len(padded_audio) / self.sample_rate
                        })
                        
                        # Send for STT processing with utterance tracking
                        print(f"üì§ [AUDIO] Sending utterance {current_utterance_id} to STT ({len(padded_audio)/self.sample_rate:.1f}s)")
                        self.stt_queue.put({
                            'audio_data': padded_audio.astype(np.float32),
                            'utterance_id': current_utterance_id,
                            'segment_id': utterance_segment_id
                        })
                    
                    # Clear buffer for next utterance
                    utterance_buffer = []
                    current_utterance_id = None
                    
                elif boundary_type == "utterance_too_short":
                    # Still save short utterances for completeness
                    if utterance_buffer and current_utterance_id:
                        short_segment_id = self.save_audio_segment(
                            np.array(utterance_buffer),
                            {"type": "short_utterance", "utterance_id": current_utterance_id}
                        )
                        self.update_utterance_status(current_utterance_id, "too_short", {
                            'end_time': time.time(),
                            'audio_segment_ids': [short_segment_id]
                        })
                    
                    utterance_buffer = []
                    current_utterance_id = None
                
                # Run periodic cleanup
                if len(utterance_buffer) % 100 == 0:  # Every 100 chunks
                    self.run_automatic_cleanup()
                
                # Prevent excessive memory usage while preserving all audio
                max_buffer_samples = int(30 * self.sample_rate)
                if len(utterance_buffer) > max_buffer_samples:
                    # Save overflow audio before trimming
                    overflow_audio = np.array(utterance_buffer[:-max_buffer_samples])
                    overflow_segment_id = self.save_audio_segment(
                        overflow_audio,
                        {"type": "overflow_segment", "utterance_id": current_utterance_id}
                    )
                    
                    # Keep only recent audio in buffer
                    utterance_buffer = utterance_buffer[-max_buffer_samples:]
                    
                    if current_utterance_id:
                        # Update utterance record with overflow segment
                        record = self.utterance_queue.get(current_utterance_id)
                        if record:
                            record.audio_segment_ids.append(overflow_segment_id)
                    
            except queue.Empty:
                continue
            except Exception as e:
                print(f"‚ö†Ô∏è [AUDIO] Processing error: {e}")
                # Even on error, try to save any buffered audio
                if utterance_buffer:
                    error_segment_id = self.save_audio_segment(
                        np.array(utterance_buffer),
                        {"type": "error_recovery", "error": str(e)}
                    )
                    print(f"üíæ [RECOVERY] Saved error audio segment {error_segment_id}")
    
    def process_stt(self):
        """Process STT with guaranteed zero-loss transcription"""
        while True:
            try:
                stt_data = self.stt_queue.get(timeout=1.0)
                
                # Handle both old format (raw audio) and new format (dict with metadata)
                if isinstance(stt_data, dict):
                    audio_chunk = stt_data['audio_data']
                    utterance_id = stt_data['utterance_id']
                    segment_id = stt_data['segment_id']
                else:
                    # Legacy format - create tracking for untracked audio
                    audio_chunk = stt_data
                    utterance_id = self.create_utterance_record()
                    segment_id = self.save_audio_segment(audio_chunk, {"type": "legacy_stt"})
                    self.update_utterance_status(utterance_id, "transcribing")
                
                print(f"üéØ [STT] Processing utterance {utterance_id} (segment {segment_id})")
                
                try:
                    # Run Whisper STT
                    segments, _ = self.whisper_model.transcribe(audio_chunk, language="en")
                    
                    text_segments = []
                    for segment in segments:
                        text_segments.append(segment.text.strip())
                    
                    if text_segments:
                        full_text = " ".join(text_segments).strip()
                        
                        # Update utterance record with transcription
                        self.update_utterance_status(utterance_id, "transcribed", {
                            'raw_transcription': full_text,
                            'cleaned_transcription': full_text  # Can add cleaning later
                        })
                        
                        # Mark associated audio segment as processed
                        if segment_id and segment_id in self.audio_segments:
                            self.audio_segments[segment_id].processed = True
                            self.audio_segments[segment_id].transcription = full_text
                        
                        if full_text and len(full_text) > 0:  # Accept ALL transcriptions in medical mode
                            print(f"üéØ [STT] {utterance_id}: {full_text}")
                            
                            if self.medical_mode:
                                # In medical mode, NEVER skip transcriptions
                                print(f"üè• [MEDICAL] Guaranteed processing: {full_text}")
                                self.add_to_recent_transcripts(full_text)
                                self.llm_queue.put({
                                    'text': full_text,
                                    'utterance_id': utterance_id,
                                    'timestamp': time.time(),
                                    'priority': 'medical'
                                })
                                self.update_utterance_status(utterance_id, "llm_queued")
                            else:
                                # Legacy mode with deduplication (NOT recommended for medical)
                                if not self.is_similar_transcript(full_text):
                                    current_time = time.time()
                                    if current_time - self.last_response_time >= self.min_response_gap:
                                        print(f"üéØ [STT]: {full_text}")
                                        self.add_to_recent_transcripts(full_text)
                                        self.llm_queue.put({
                                            'text': full_text,
                                            'utterance_id': utterance_id,
                                            'timestamp': time.time()
                                        })
                                        self.update_utterance_status(utterance_id, "llm_queued")
                                    else:
                                        print(f"‚ö†Ô∏è [STT]: Rate limited (too soon): {full_text}")
                                        # Still queue it with lower priority
                                        self.llm_queue.put({
                                            'text': full_text,
                                            'utterance_id': utterance_id,
                                            'timestamp': time.time(),
                                            'priority': 'low'
                                        })
                                        self.update_utterance_status(utterance_id, "llm_queued_low_priority")
                                else:
                                    print(f"‚ö†Ô∏è [STT]: Similar to recent, but queuing anyway: {full_text}")
                                    # Queue duplicates with low priority for medical completeness
                                    self.llm_queue.put({
                                        'text': full_text,
                                        'utterance_id': utterance_id,
                                        'timestamp': time.time(),
                                        'priority': 'duplicate'
                                    })
                                    self.update_utterance_status(utterance_id, "llm_queued_duplicate")
                        else:
                            # Empty transcription - still track it
                            self.update_utterance_status(utterance_id, "empty_transcription")
                            print(f"üîá [STT] {utterance_id}: Empty transcription")
                    else:
                        # No segments detected - still track it
                        self.update_utterance_status(utterance_id, "no_speech_detected")
                        print(f"üîá [STT] {utterance_id}: No speech detected")
                
                except Exception as stt_error:
                    # STT failed - save error and retry later
                    error_msg = f"STT processing failed: {stt_error}"
                    self.update_utterance_status(utterance_id, "stt_error", {
                        'error_log': [error_msg]
                    })
                    print(f"‚ùå [STT] {utterance_id}: {error_msg}")
                    
                    # In medical mode, queue for manual review/retry
                    if self.medical_mode:
                        self.llm_queue.put({
                            'text': f"[STT_ERROR] Audio segment {segment_id} failed transcription",
                            'utterance_id': utterance_id,
                            'timestamp': time.time(),
                            'priority': 'error_review',
                            'error': error_msg
                        })
                
            except queue.Empty:
                continue
            except Exception as e:
                print(f"‚ùå [STT] Queue processing error: {e}")
    
    def process_llm(self):
        """Process LLM responses with rate limiting and circuit breaker"""
        conversation_history = []
        
        while True:
            try:
                # Check circuit breaker status
                current_time = time.time()
                if self.llm_health_status == "failed":
                    if current_time - self.circuit_breaker_reset_time > self.circuit_breaker_timeout:
                        print("üîÑ [LLM] Attempting to reset circuit breaker...")
                        self.llm_health_status = "degraded"
                        self.llm_consecutive_errors = 0
                    else:
                        # Still in circuit breaker timeout
                        time.sleep(1.0)
                        continue
                
                # Try to get a request with timeout
                try:
                    llm_data = self.llm_queue.get(timeout=0.5)
                except queue.Empty:
                    # Check for pending consolidation and backlog processing
                    self._process_pending_requests(conversation_history)
                    self._process_backlog_if_healthy(conversation_history)
                    continue
                
                # Handle both old format (string) and new format (dict)
                if isinstance(llm_data, dict):
                    user_text = llm_data['text']
                    utterance_id = llm_data.get('utterance_id')
                    priority = llm_data.get('priority', 'normal')
                    timestamp = llm_data.get('timestamp', current_time)
                else:
                    # Legacy string format
                    user_text = llm_data
                    utterance_id = None
                    priority = 'normal'
                    timestamp = current_time
                
                # Check for exit commands
                if any(word in user_text.lower() for word in ['quit', 'exit', 'stop', 'goodbye']):
                    print("üëã Exit command detected")
                    print("ü§ñ [LLM]: Goodbye!")
                    self.stop_audio_stream()
                    break
                
                # Add to pending requests for consolidation with tracking
                request_data = {
                    'text': user_text,
                    'timestamp': timestamp,
                    'utterance_id': utterance_id,
                    'priority': priority
                }
                self.pending_llm_requests.append(request_data)
                
                if utterance_id:
                    self.update_utterance_status(utterance_id, "llm_pending")
                
                # In medical mode, never drop requests
                if self.medical_mode:
                    # No size limit in medical mode - process when ready
                    pass
                else:
                    # Legacy mode - limit queue size
                    if len(self.pending_llm_requests) > 10:  # Increased limit
                        print("‚ö†Ô∏è [LLM] Queue getting large, prioritizing processing")
                        # Process immediately instead of dropping
                
                # Process if enough time has passed or queue is getting full
                time_since_last = current_time - self.last_llm_request_time
                should_process = (
                    time_since_last >= self.min_llm_gap or
                    len(self.pending_llm_requests) >= self.llm_queue_max_size - 1
                )
                
                if should_process:
                    self._process_pending_requests(conversation_history)
                
            except Exception as e:
                print(f"‚ö†Ô∏è [LLM] Processing error: {e}")
                self._handle_llm_error()
    
    def _process_pending_requests(self, conversation_history):
        """Process and consolidate pending LLM requests"""
        if not self.pending_llm_requests:
            return
        
        current_time = time.time()
        
        # Check if we should wait longer for consolidation
        latest_request_time = max(req['timestamp'] for req in self.pending_llm_requests)
        if current_time - latest_request_time < self.llm_request_consolidation_window:
            return
        
        # Check rate limiting
        if current_time - self.last_llm_request_time < self.min_llm_gap:
            return
        
        # In medical mode, NEVER drop requests - move to backlog instead
        if self.llm_health_status == "failed":
            if self.medical_mode:
                print("üè• [MEDICAL] LLM failed - moving requests to persistent backlog")
                # Move all pending requests to persistent backlog
                for req in self.pending_llm_requests:
                    self.llm_backlog.append(req)
                    if 'utterance_id' in req:
                        self.update_utterance_status(req['utterance_id'], "llm_backlogged")
                self.pending_llm_requests.clear()
                return
            else:
                print("üö´ [LLM] Circuit breaker open, dropping requests")
                self.pending_llm_requests.clear()
                return
        
        # Consolidate multiple requests into one
        consolidated_text = self._consolidate_requests()
        if not consolidated_text:
            return
        
        print(f"üîÑ [LLM] Processing consolidated request ({len(self.pending_llm_requests)} utterances)")
        self.pending_llm_requests.clear()
        
        # Add to conversation history
        conversation_history.append(f"User: {consolidated_text}")
        
        # Keep only recent history to avoid context overflow
        if len(conversation_history) > 6:
            conversation_history = conversation_history[-6:]
        
        # Generate response using Ollama with protection
        self._make_llm_request(consolidated_text, conversation_history)
    
    def _consolidate_requests(self):
        """Consolidate multiple pending requests into a single coherent request"""
        if not self.pending_llm_requests:
            return ""
        
        # Extract unique text from requests
        texts = []
        for req in self.pending_llm_requests:
            text = req['text'].strip()
            if text and text not in texts:
                texts.append(text)
        
        # Join with natural separators
        if len(texts) == 1:
            return texts[0]
        else:
            return ". ".join(texts) + "."
    
    def _make_llm_request(self, user_text, conversation_history):
        """Make LLM request with error handling and health monitoring"""
        try:
            self.last_llm_request_time = time.time()
            
            # Create conversation context
            if len(conversation_history) > 1:
                context = "\n".join(conversation_history[-4:])  # Recent context
                prompt = f"{context}\nAssistant:"
            else:
                prompt = f"User: {user_text}\nAssistant:"
            
            # Adjust timeout based on health status
            timeout = 15 if self.llm_health_status == "degraded" else 10
            
            print(f"ü§ñ [LLM] Sending request (health: {self.llm_health_status})...")
            
            # Call Ollama API
            response = requests.post(self.ollama_url, 
                json={
                    "model": self.model_name,
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.7,
                        "top_p": 0.9,
                        "max_tokens": 100
                    }
                },
                timeout=timeout)
            
            if response.status_code == 200:
                result = response.json()
                llm_response = result.get("response", "").strip()
                
                # Clean up response
                llm_response = self.clean_response(llm_response)
                
                if llm_response:
                    conversation_history.append(f"Assistant: {llm_response}")
                    print(f"ü§ñ [LLM]: {llm_response}")
                    
                    # Update last response time
                    self.last_response_time = time.time()
                    
                    # Reset error tracking on success
                    self._handle_llm_success()
                else:
                    print("ü§ñ [LLM]: Empty response received")
                    self._handle_llm_error()
            else:
                print(f"ü§ñ [LLM]: Server error (status: {response.status_code})")
                self._handle_llm_error()
        
        except requests.exceptions.Timeout:
            print("ü§ñ [LLM]: Request timeout - Ollama may be overloaded")
            self._handle_llm_error()
        except Exception as e:
            print(f"ü§ñ [LLM]: Error - {e}")
            self._handle_llm_error()
    
    def _handle_llm_success(self):
        """Handle successful LLM response"""
        self.llm_consecutive_errors = 0
        if self.llm_health_status == "degraded":
            self.llm_health_status = "healthy"
            print("‚úÖ [LLM] Health restored to normal")
    
    def _handle_llm_error(self):
        """Handle LLM errors with circuit breaker logic"""
        self.llm_error_count += 1
        self.llm_consecutive_errors += 1
        
        if self.llm_consecutive_errors >= self.max_consecutive_errors:
            print(f"üö´ [LLM] Circuit breaker activated after {self.max_consecutive_errors} consecutive errors")
            self.llm_health_status = "failed"
            self.circuit_breaker_reset_time = time.time()
            self.pending_llm_requests.clear()  # Drop pending requests
        elif self.llm_consecutive_errors >= 2:
            self.llm_health_status = "degraded"
            print("‚ö†Ô∏è [LLM] Health degraded - reducing request frequency")
        
        # Always provide fallback response
        print("ü§ñ [LLM]: I'm having trouble responding right now. Please try again in a moment.")
    
    def _process_backlog_if_healthy(self, conversation_history):
        """Process backlogged requests when LLM is healthy again"""
        if self.llm_health_status == "healthy" and self.llm_backlog:
            print(f"üîÑ [MEDICAL] Processing {len(self.llm_backlog)} backlogged requests")
            
            # Process oldest backlogged requests first
            while self.llm_backlog and len(self.pending_llm_requests) < 3:
                backlog_req = self.llm_backlog.pop(0)
                self.pending_llm_requests.append(backlog_req)
                
                if 'utterance_id' in backlog_req and backlog_req['utterance_id']:
                    self.update_utterance_status(backlog_req['utterance_id'], "llm_pending_from_backlog")
            
            # Process if we have requests
            if self.pending_llm_requests:
                self._process_pending_requests(conversation_history)
    
    def get_zero_loss_status(self):
        """Get comprehensive status showing no audio loss"""
        with self.audio_log_lock:
            total_audio_time = sum(seg.duration for seg in self.audio_segments.values())
            
            status_summary = {
                'session_id': self.session_id,
                'medical_mode': self.medical_mode,
                'total_audio_segments': len(self.audio_segments),
                'total_audio_duration_seconds': total_audio_time,
                'total_utterances_tracked': len(self.utterance_queue),
                'pending_llm_requests': len(self.pending_llm_requests),
                'backlogged_requests': len(self.llm_backlog),
                'llm_health': self.llm_health_status,
                'processing_pipeline_status': {}
            }
            
            # Count utterances by status
            for utterance_id, record in self.utterance_queue.items():
                status = record.status
                if status not in status_summary['processing_pipeline_status']:
                    status_summary['processing_pipeline_status'][status] = 0
                status_summary['processing_pipeline_status'][status] += 1
            
            # Check for any potential loss
            unprocessed_count = sum(1 for r in self.utterance_queue.values() 
                                  if r.status in ['detecting', 'transcribing', 'stt_error'])
            backlog_count = len(self.llm_backlog)
            
            status_summary['zero_loss_guarantee'] = {
                'all_audio_saved': True,  # We save every chunk
                'unprocessed_utterances': unprocessed_count,
                'backlogged_utterances': backlog_count,
                'total_at_risk': unprocessed_count + backlog_count,
                'loss_risk_level': 'NONE' if (unprocessed_count + backlog_count) == 0 else 'MONITORED'
            }
            
            return status_summary
    
    def save_session_summary(self):
        """Save complete session summary for medical records"""
        summary = self.get_zero_loss_status()
        summary_file = os.path.join(self.audio_log_dir, "session_summary.json")
        
        try:
            with open(summary_file, 'w') as f:
                json.dump(summary, f, indent=2, default=str)
            print(f"üíæ [MEDICAL] Session summary saved: {summary_file}")
        except Exception as e:
            print(f"‚ö†Ô∏è [MEDICAL] Could not save session summary: {e}")
        
        return summary
    
    def configure_storage(self, mode=None, max_age_hours=None, max_storage_mb=None, keep_raw=None):
        """Configure storage management settings"""
        if mode:
            self.storage_mode = mode
            print(f"üíæ Storage mode set to: {mode.upper()}")
            
            # Apply mode-specific defaults
            if mode == "medical":
                self.max_session_age_hours = 7 * 24  # 7 days
                self.keep_raw_chunks = True
                self.max_total_storage_mb = 5000  # 5GB for medical
            elif mode == "standard":
                self.max_session_age_hours = 24  # 1 day
                self.keep_raw_chunks = False
                self.max_total_storage_mb = 1000  # 1GB
            elif mode == "minimal":
                self.max_session_age_hours = 2  # 2 hours
                self.keep_raw_chunks = False
                self.max_total_storage_mb = 100  # 100MB
        
        if max_age_hours is not None:
            self.max_session_age_hours = max_age_hours
            print(f"üíæ Session retention set to: {max_age_hours} hours")
            
        if max_storage_mb is not None:
            self.max_total_storage_mb = max_storage_mb
            print(f"üíæ Storage limit set to: {max_storage_mb} MB")
            
        if keep_raw is not None:
            self.keep_raw_chunks = keep_raw
            print(f"üíæ Keep raw chunks: {'YES' if keep_raw else 'NO'}")
    
    def get_storage_usage(self):
        """Calculate current storage usage"""
        total_size = 0
        file_count = 0
        
        try:
            # Check all audio session directories
            base_dir = "audio_sessions"
            if os.path.exists(base_dir):
                for session_dir in os.listdir(base_dir):
                    session_path = os.path.join(base_dir, session_dir)
                    if os.path.isdir(session_path):
                        for file in os.listdir(session_path):
                            file_path = os.path.join(session_path, file)
                            if os.path.isfile(file_path):
                                total_size += os.path.getsize(file_path)
                                file_count += 1
            
            total_mb = total_size / (1024 * 1024)
            return {
                'total_mb': total_mb,
                'total_files': file_count,
                'limit_mb': self.max_total_storage_mb,
                'usage_percent': (total_mb / self.max_total_storage_mb) * 100,
                'needs_cleanup': total_mb > self.max_total_storage_mb * 0.8  # 80% threshold
            }
        except Exception as e:
            print(f"‚ö†Ô∏è [STORAGE] Error calculating usage: {e}")
            return {'total_mb': 0, 'total_files': 0, 'needs_cleanup': False}
    
    def cleanup_old_sessions(self, dry_run=False):
        """Clean up old audio sessions based on retention policy"""
        try:
            base_dir = "audio_sessions"
            if not os.path.exists(base_dir):
                return {'deleted_sessions': 0, 'freed_mb': 0}
            
            current_time = time.time()
            max_age_seconds = self.max_session_age_hours * 3600
            deleted_sessions = 0
            freed_bytes = 0
            
            for session_dir in os.listdir(base_dir):
                session_path = os.path.join(base_dir, session_dir)
                if os.path.isdir(session_path) and session_dir != self.session_id:  # Don't delete current session
                    
                    # Check session age
                    session_age = current_time - os.path.getctime(session_path)
                    
                    if session_age > max_age_seconds:
                        # Calculate size before deletion
                        session_size = 0
                        for file in os.listdir(session_path):
                            file_path = os.path.join(session_path, file)
                            if os.path.isfile(file_path):
                                session_size += os.path.getsize(file_path)
                        
                        if not dry_run:
                            # Delete the session
                            import shutil
                            shutil.rmtree(session_path)
                            print(f"üóëÔ∏è [CLEANUP] Deleted old session: {session_dir} ({session_size/(1024*1024):.1f}MB)")
                        else:
                            print(f"üîç [DRY RUN] Would delete: {session_dir} ({session_size/(1024*1024):.1f}MB)")
                        
                        deleted_sessions += 1
                        freed_bytes += session_size
            
            freed_mb = freed_bytes / (1024 * 1024)
            self.storage_stats['cleanup_count'] += 1
            self.storage_stats['files_deleted'] += deleted_sessions
            
            return {
                'deleted_sessions': deleted_sessions,
                'freed_mb': freed_mb,
                'dry_run': dry_run
            }
            
        except Exception as e:
            print(f"‚ö†Ô∏è [CLEANUP] Error during cleanup: {e}")
            return {'deleted_sessions': 0, 'freed_mb': 0, 'error': str(e)}
    
    def cleanup_processed_chunks(self):
        """Clean up raw audio chunks after successful processing"""
        if self.keep_raw_chunks or self.storage_mode == "medical":
            return  # Don't cleanup in medical mode or if configured to keep
        
        try:
            deleted_count = 0
            freed_bytes = 0
            
            for segment_id, segment in list(self.audio_segments.items()):
                # Only delete if it was successfully processed
                if segment.processed and segment.transcription:
                    segment_file = os.path.join(self.audio_log_dir, f"segment_{segment_id:06d}.pkl")
                    wav_file = os.path.join(self.audio_log_dir, f"segment_{segment_id:06d}.wav")
                    
                    for file_path in [segment_file, wav_file]:
                        if os.path.exists(file_path):
                            file_size = os.path.getsize(file_path)
                            os.remove(file_path)
                            freed_bytes += file_size
                            deleted_count += 1
                    
                    # Remove from memory too
                    del self.audio_segments[segment_id]
            
            if deleted_count > 0:
                print(f"üßπ [CLEANUP] Deleted {deleted_count} processed chunks, freed {freed_bytes/(1024*1024):.1f}MB")
            
        except Exception as e:
            print(f"‚ö†Ô∏è [CLEANUP] Error cleaning processed chunks: {e}")
    
    def run_automatic_cleanup(self):
        """Run automatic cleanup if needed"""
        current_time = time.time()
        
        # Check if it's time for cleanup
        if current_time - self.last_cleanup_time < (self.cleanup_interval_minutes * 60):
            return
        
        self.last_cleanup_time = current_time
        
        # Get current storage usage
        usage = self.get_storage_usage()
        
        print(f"üîç [STORAGE] Usage: {usage['total_mb']:.1f}MB / {usage['limit_mb']}MB ({usage['usage_percent']:.1f}%)")
        
        # Run cleanup if needed
        if usage['needs_cleanup']:
            print("üßπ [STORAGE] Running automatic cleanup...")
            
            # Clean up processed chunks first
            self.cleanup_processed_chunks()
            
            # Then clean up old sessions if still over limit
            usage_after = self.get_storage_usage()
            if usage_after['needs_cleanup']:
                cleanup_result = self.cleanup_old_sessions()
                print(f"üßπ [CLEANUP] Deleted {cleanup_result['deleted_sessions']} old sessions, freed {cleanup_result['freed_mb']:.1f}MB")
        
        # Update storage stats
        self.storage_stats['total_storage_mb'] = usage['total_mb']
    
    def clean_response(self, response):
        """Clean and improve LLM response quality"""
        if not response:
            return ""
        
        # Remove common artifacts and prefixes
        response = response.replace("Assistant:", "").replace("User:", "")
        response = response.replace("<|endoftext|>", "")
        
        # Remove leading/trailing whitespace and newlines
        response = response.strip()
        
        # Take only the first sentence or two for conciseness
        sentences = response.split('.')
        if len(sentences) > 2:
            response = '. '.join(sentences[:2]) + '.'
        
        # Limit response length
        if len(response) > 150:
            response = response[:150].rsplit(' ', 1)[0] + '.'
        
        # Ensure it ends properly
        if response and not response.endswith(('.', '!', '?')):
            response += '.'
        
        return response.strip()
    
    
    async def run(self):
        """Run the full pipeline"""
        print("üöÄ Starting Offline Real-time Pipeline")
        print("üìä Calibrating background noise levels...")
        print("üîá Please remain quiet for ~6 seconds for optimal noise rejection")
        print("Speak naturally - the system will respond in real-time!")
        print("Say 'quit', 'exit', or 'stop' to end the conversation.\n")
        
        # Start audio stream
        self.start_audio_stream()
        
        # Start processing threads - simplified architecture
        threads = [
            threading.Thread(target=self.process_raw_audio, daemon=True, name="AudioProcessor"),
            threading.Thread(target=self.process_stt, daemon=True, name="STTProcessor"),
            threading.Thread(target=self.process_llm, daemon=True, name="LLMProcessor"),
        ]
        
        for thread in threads:
            thread.start()
        
        print("‚úÖ All processing threads started")
        print("üé§ Listening... speak now!\n")
        
        try:
            # Keep main thread alive
            while self.is_recording:
                await asyncio.sleep(0.5)
                
        except KeyboardInterrupt:
            print("\nüõë Interrupted by user")
        finally:
            self.stop_audio_stream()
            self._shutdown_with_summary()
            print("üîö Pipeline stopped")
    
    def _shutdown_with_summary(self):
        """Shutdown with complete zero-loss verification"""
        print("\nüìä SHUTDOWN SUMMARY - Zero Loss Verification")
        print("=" * 50)
        
        # Save final session summary
        summary = self.save_session_summary()
        
        # Print zero-loss status
        zero_loss = summary['zero_loss_guarantee']
        print(f"üè• Medical Mode: {'ENABLED' if summary['medical_mode'] else 'DISABLED'}")
        print(f"üìÅ Session ID: {summary['session_id']}")
        print(f"‚è±Ô∏è Total Audio Duration: {summary['total_audio_duration_seconds']:.1f} seconds")
        print(f"üé§ Audio Segments Saved: {summary['total_audio_segments']}")
        print(f"üó£Ô∏è Utterances Tracked: {summary['total_utterances_tracked']}")
        print(f"üìã Pending LLM Requests: {summary['pending_llm_requests']}")
        print(f"üì¶ Backlogged Requests: {summary['backlogged_requests']}")
        print(f"‚ù§Ô∏è LLM Health: {summary['llm_health']}")
        
        # Storage summary
        storage_usage = self.get_storage_usage()
        print(f"\nüíæ STORAGE SUMMARY:")
        print(f"   Current Usage: {storage_usage['total_mb']:.1f}MB / {storage_usage['limit_mb']}MB ({storage_usage['usage_percent']:.1f}%)")
        print(f"   Total Files: {storage_usage['total_files']}")
        print(f"   Storage Mode: {self.storage_mode.upper()}")
        print(f"   Retention: {self.max_session_age_hours} hours")
        print(f"   Keep Raw Chunks: {'YES' if self.keep_raw_chunks else 'NO'}")
        print(f"   Cleanup Count: {self.storage_stats['cleanup_count']}")
        print(f"   Files Deleted: {self.storage_stats['files_deleted']}")
        
        print(f"\nüîí ZERO LOSS GUARANTEE STATUS:")
        print(f"   All Audio Saved: {'‚úÖ YES' if zero_loss['all_audio_saved'] else '‚ùå NO'}")
        print(f"   Unprocessed Utterances: {zero_loss['unprocessed_utterances']}")
        print(f"   Backlogged Utterances: {zero_loss['backlogged_utterances']}")
        print(f"   Total At Risk: {zero_loss['total_at_risk']}")
        print(f"   Loss Risk Level: {zero_loss['loss_risk_level']}")
        
        if zero_loss['total_at_risk'] == 0:
            print("‚úÖ ZERO AUDIO LOSS CONFIRMED - All utterances processed")
        else:
            print(f"‚ö†Ô∏è {zero_loss['total_at_risk']} utterances require follow-up processing")
            print(f"üìÅ All audio saved in: {self.audio_log_dir}")
            print("üí° Use replay functionality to process remaining utterances")
        
        print("=" * 50)
        
        # Final cleanup if not in medical mode
        if self.storage_mode != "medical":
            print("\nüßπ Running final cleanup...")
            final_cleanup = self.cleanup_old_sessions(dry_run=False)
            if final_cleanup['deleted_sessions'] > 0:
                print(f"üßπ Final cleanup freed {final_cleanup['freed_mb']:.1f}MB from {final_cleanup['deleted_sessions']} old sessions")

def create_pipeline_with_storage_config(medical_mode=True, storage_mode=None, max_age_hours=None, max_storage_mb=None):
    """Convenience function to create pipeline with custom storage settings"""
    pipeline = OfflineRealtimePipeline()
    
    # Override medical mode if specified
    if not medical_mode:
        pipeline.medical_mode = False
    
    # Configure storage if parameters provided
    if storage_mode or max_age_hours or max_storage_mb:
        pipeline.configure_storage(
            mode=storage_mode,
            max_age_hours=max_age_hours,
            max_storage_mb=max_storage_mb,
            keep_raw=medical_mode  # Keep raw chunks in medical mode
        )
    
    return pipeline

async def main():
    pipeline = OfflineRealtimePipeline()
    await pipeline.run()

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except Exception as e:
        print(f"Pipeline error: {e}")
    finally:
        print("üëã Goodbye!")